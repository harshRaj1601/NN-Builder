{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37313449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Set the API key for Generative AI\n",
    "genai.configure(api_key=os.getenv(\"GENAI_API_KEY\"))\n",
    "\n",
    "# Define the prompt for the model\n",
    "prompt = \"\"\"Given a dataset with following properties:\n",
    "        - Number of samples: 1000 images\n",
    "        - Image size: 320x240 pixels\n",
    "        - Task type: classification\n",
    "        - Number of classes: 8\n",
    "\n",
    "        Suggest an optimal neural network architecture for this classification task.  Provide the response as a *pure*, valid JSON string with *no* surrounding text or explanations. Do start with \"{\" and do not use newline codes or tabs. The JSON should have a \"layers\" field, an \"optimizer\" field, a \"learning_rate\" field, a \"loss\" field, a \"metrics\" field, a \"batch_size\" field, and an \"epochs\" field.\n",
    "        \"\"\"\n",
    "\n",
    "# Call the Generative AI model with the prompt\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.0-flash-exp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3158016",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.generate_content(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a65533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n  \"layers\": [\\n    {\"type\": \"Conv2D\", \"filters\": 32, \"kernel_size\": [3, 3], \"activation\": \"relu\", \"input_shape\": [320, 240, 3]},\\n    {\"type\": \"MaxPooling2D\", \"pool_size\": [2, 2]},\\n    {\"type\": \"Conv2D\", \"filters\": 64, \"kernel_size\": [3, 3], \"activation\": \"relu\"},\\n    {\"type\": \"MaxPooling2D\", \"pool_size\": [2, 2]},\\n    {\"type\": \"Conv2D\", \"filters\": 128, \"kernel_size\": [3, 3], \"activation\": \"relu\"},\\n    {\"type\": \"MaxPooling2D\", \"pool_size\": [2, 2]},\\n    {\"type\": \"Flatten\"},\\n    {\"type\": \"Dense\", \"units\": 128, \"activation\": \"relu\"},\\n    {\"type\": \"Dropout\", \"rate\": 0.5},\\n    {\"type\": \"Dense\", \"units\": 8, \"activation\": \"softmax\"}\\n  ],\\n  \"optimizer\": \"Adam\",\\n  \"learning_rate\": 0.001,\\n  \"loss\": \"categorical_crossentropy\",\\n  \"metrics\": [\"accuracy\"],\\n  \"batch_size\": 32,\\n  \"epochs\": 30\\n}\\n```'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a88c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{  \"layers\": [    {\"type\": \"Conv2D\", \"filters\": 32, \"kernel_size\": [3, 3], \"activation\": \"relu\", \"input_shape\": [320, 240, 3]},    {\"type\": \"MaxPooling2D\", \"pool_size\": [2, 2]},    {\"type\": \"Conv2D\", \"filters\": 64, \"kernel_size\": [3, 3], \"activation\": \"relu\"},    {\"type\": \"MaxPooling2D\", \"pool_size\": [2, 2]},    {\"type\": \"Conv2D\", \"filters\": 128, \"kernel_size\": [3, 3], \"activation\": \"relu\"},    {\"type\": \"MaxPooling2D\", \"pool_size\": [2, 2]},    {\"type\": \"Flatten\"},    {\"type\": \"Dense\", \"units\": 128, \"activation\": \"relu\"},    {\"type\": \"Dropout\", \"rate\": 0.5},    {\"type\": \"Dense\", \"units\": 8, \"activation\": \"softmax\"}  ],  \"optimizer\": \"Adam\",  \"learning_rate\": 0.001,  \"loss\": \"categorical_crossentropy\",  \"metrics\": [\"accuracy\"],  \"batch_size\": 32,  \"epochs\": 30}\n",
      "{'layers': [{'type': 'Conv2D', 'filters': 32, 'kernel_size': [3, 3], 'activation': 'relu', 'input_shape': [320, 240, 3]}, {'type': 'MaxPooling2D', 'pool_size': [2, 2]}, {'type': 'Conv2D', 'filters': 64, 'kernel_size': [3, 3], 'activation': 'relu'}, {'type': 'MaxPooling2D', 'pool_size': [2, 2]}, {'type': 'Conv2D', 'filters': 128, 'kernel_size': [3, 3], 'activation': 'relu'}, {'type': 'MaxPooling2D', 'pool_size': [2, 2]}, {'type': 'Flatten'}, {'type': 'Dense', 'units': 128, 'activation': 'relu'}, {'type': 'Dropout', 'rate': 0.5}, {'type': 'Dense', 'units': 8, 'activation': 'softmax'}], 'optimizer': 'Adam', 'learning_rate': 0.001, 'loss': 'categorical_crossentropy', 'metrics': ['accuracy'], 'batch_size': 32, 'epochs': 30}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "result = res.text.strip()\n",
    "result = result.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "result = result.replace(\"```json\\n\", \"\").replace(\"```\", \"\").replace(\"json{\", \"{\").replace(\"}\", \"}\")\n",
    "print(result)\n",
    "response = json.loads(result)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f137be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with shape:\n",
      "Dataset saved to classification_dataset.csv\n",
      "Labels (y): (5000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "\n",
    "# Create a random classification dataset\n",
    "X, y = make_classification(n_samples=5000, n_features=20, n_classes=4, n_informative=8, shuffle=True)\n",
    "\n",
    "print(\"Dataset created with shape:\")\n",
    "# Save the dataset into a CSV file\n",
    "df = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(X.shape[1])])\n",
    "df['Label'] = y\n",
    "df.to_csv(\"classification_dataset.csv\", index=False)\n",
    "print(\"Dataset saved to classification_dataset.csv\")\n",
    "print(\"Labels (y):\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb0299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess your data\n",
    "def preprocess_data(df, target_column):\n",
    "    # Split features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Handle categorical features\n",
    "    for col in X.select_dtypes(include=['object']).columns:\n",
    "        X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    # Handle categorical target\n",
    "    if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    return X_scaled, y, scaler\n",
    "\n",
    "# Build model with your custom architecture\n",
    "def build_model(input_dim=4):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(3, activation='linear', input_shape=(input_dim,)))\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss=mse, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Train model\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs=20, batch_size=32):\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# Make predictions\n",
    "def predict(model, X_new, scaler):\n",
    "    # Scale new data using the same scaler\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    predictions = model.predict(X_new_scaled)\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    df = pd.read_csv('your_data.csv')\n",
    "\n",
    "    # Preprocess\n",
    "    X, y, scaler = preprocess_data(df, 'target_column')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_model(input_dim=X.shape[1])  # Pass input dimension dynamically\n",
    "    history = train_model(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate_model(model, X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    model.save('model.h5')\n",
    "    \n",
    "    # Quick model summary\n",
    "    model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
